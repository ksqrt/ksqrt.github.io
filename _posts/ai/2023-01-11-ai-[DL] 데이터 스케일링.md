---
layout: single
title: "[DL] 데이터 스케일링"
categories:
  - ai

toc: ture
toc_sticky: true
---

<!-- 위는 머릿말임 아래부터 포스트 본문 -->

### 1. 데이터 스케일링

데이터 스케일링(data scaling)은 데이터의 값을 정규화하는 과정입니다. 

이를 수행하는 이유는 일부 머신러닝 알고리즘이나 기술들이 값의 크기(스케일) 에 민감하기 때문입니다. 일반적으로는 데이터를 특정 범위(예를 들어 0과 1 사이)로 정규화 하거나 표준화(표준 정규분호)하는 방법을 사용합니다.

### 2. 정규화 (normalization)

- 데이터를 주로 0과 1 사이로 정규화 합니다.
- 데이터들이 비슷한 수준의 스케일로 변환 되기 때문에 모델이 큰 값에 더 민감해지는것을 방지 할 수 있습니다.

```python
from sklearn.preprocessing import MinMaxScaler
# x_train 을 스케일링
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_scaled = scaler.transform(x_train)
# x_test 를 스케일링
x_test = scaler.transform(x_test)
```

### 3. 표준화 (normalization)

- 데이터를 평균이 0, 분산이 1인 표준 정규 분포로 변환하는 과정입니다.
- 마찬가지로 데이터들이 비슷한 수준의 스케일로 변환 되기 때문에
    
    모델이 큰 값에 더 민감해지는것을 방지 할 수 있습니다.
    

```python
from sklearn.preprocessing import StandardScaler
# x_train 을 스케일링
scaler = StandardScaler()
scaler.fit(x_train)
x_train_scaled = scaler.transform(x_train)
# x_test 를 스케일링
x_test = scaler.transform(x_test)
```

### 4. 추가

데이터를 train_test_split 로 나눈 이후 스케일링을 진행하게 되는데

이떄 train 을 먼저 스케일링 한 다음에

 train 의 범위에 맞춰 test 를  transform 합니다. 

(가끔 예외가 발생하기 떄문에)

요약 : 스케일링은 무조건 train 만 하고 transform 은 train 과  test 둘 다 진행한다.