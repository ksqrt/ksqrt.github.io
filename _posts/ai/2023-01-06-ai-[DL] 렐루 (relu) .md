---
layout: single
title: "[DL] 렐루 (relu)"
categories:
  - ai

toc: ture
toc_sticky: true
---

<!-- 위는 머릿말임 아래부터 포스트 본문 -->

### 1.렐루

ReLU(Rectified Linear Unit)은 인공 신경망에서 자주 사용하는 활성화 함수입니다. ReLU는 입력이 0을 넘으면 그 입력 그대로 출력하고, 0 이하이면 0을 출력합니다. 기술적으로 정의하면 다음과 같습니다.

### 2. 식 과 그래프

```python
ReLu(x)=max(0, x)
```

![https://blog.kakaocdn.net/dn/bUd9Yo/btqUJXJGVFL/5MkBflEPM7gyPYOBIg5Sq0/img.png](https://blog.kakaocdn.net/dn/bUd9Yo/btqUJXJGVFL/5MkBflEPM7gyPYOBIg5Sq0/img.png)

### 3. 은닉층에서 렐루를 왜 많이 사용할까요?

ReLU의 식은 간단하지만 인공 신경망에서 잘 작동하기 때문에 자주 사용됩니다. ReLU는 입력이 음수일 때 출력이 0이기 때문에 음수값을 제거하는 효과가 있습니다. 따라서 이는 기울기소실 문제를 발생시키지 않습니다.

ReLU의 장점으로는 간단한 공식을 사용하다 보니 계산 속도가 빠르고, 

기울기가 1로 일정하므로 가중치 초기값에 민감하지 않다는 것이 있습니다. 

하지만 ReLU는 일부 입력 값에 대해 출력이 0이 될 수 있기 때문에

해당 뉴런이 죽는 현상이 발생 할 수 있습니다.

### 4. 활성화 함수로 사용방법

```python
model = Sequential([
    Dense(10,input_dim = 8),
    Dense(10,activation="relu"),
    Dense(10,activation="relu"),
    Dense(10,activation="relu"),
    Dense(10,activation="relu"),
# 마지막 레이어는 통상 relu 넣지 않습니다.
    Dense(1)
])
model.summary()
```

keras 에서 제공하는 완전연결층인 Dense  의 활성화 함수 인자값으로 활용됩니다 

참고) activation 을 지정하지않으면 default 값인 activation = “linear” 입니다