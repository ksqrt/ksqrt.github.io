---
layout: single
title: "[DL] 배치(batch)"
categories:
  - ai

toc: ture
toc_sticky: true
---

<!-- 위는 머릿말임 아래부터 포스트 본문 -->

배치 사이즈 (한번에 학습되는 데이터의 개수)

딥러닝 모델 훈련시 배치사이즈를 조절 하여 적절한 훈련을 진행시킵니다.

이때 배치를 잘게 자르면 시간이 오래 걸리며 자원을 많이 사용 합니다.

그러나 성능은 좋아짐(무조건은 아니다)

배치사이즈를 조절 하는것이 바로 하이퍼튜닝

```python
풀배치 : 전체 데이터를 한번에 학습
미니배치 : 전체 데이터를 여러개로 나누어 학습
```

### 1. 6개의 데이터를 준비

```python
# 1. 데이터 준비 (예시)
x = np.array([1,2,3,4,5,6])
y = np.array([1,2,3,4,5,6])
```

### 2. 풀배치로 진행 (배치사이즈=6)

```python
model.compile(loss="mae",optimizer="adam")
# fit 에서 batch 사이즈를 조절해줍니다
model.fit(x,y, epochs=10,batch_size=6)
```

```python
Epoch 1/10
1/1 [==============================] - 0s 151ms/step - loss: 0.9565
Epoch 2/10
1/1 [==============================] - 0s 996us/step - loss: 0.9459
Epoch 3/10
1/1 [==============================] - 0s 2ms/step - loss: 0.9354
Epoch 4/10
1/1 [==============================] - 0s 1ms/step - loss: 0.9248
Epoch 5/10
1/1 [==============================] - 0s 997us/step - loss: 0.9143
Epoch 6/10
1/1 [==============================] - 0s 997us/step - loss: 0.9038
Epoch 7/10
1/1 [==============================] - 0s 997us/step - loss: 0.8933
Epoch 8/10
1/1 [==============================] - 0s 2ms/step - loss: 0.8828
Epoch 9/10
1/1 [==============================] - 0s 997us/step - loss: 0.8723
Epoch 10/10
1/1 [==============================] - 0s 1ms/step - loss: 0.8618
<keras.callbacks.History at 0x1f7ec338370>
```

Epoch 위의 숫자 1/1 이 보여주듯이 전체 데이터를 통으로 학습

### 3. 미니배치로 진행 (배치사이즈 =3)

```python
model.compile(loss="mae",optimizer="adam")
# fit 에서 batch 사이즈를 조절해줍니다
model.fit(x,y, epochs=10,batch_size=3)
```

```python
Epoch 1/10
2/2 [==============================] - 0s 2ms/step - loss: 1.6794
Epoch 2/10
2/2 [==============================] - 0s 2ms/step - loss: 1.6546
Epoch 3/10
2/2 [==============================] - 0s 2ms/step - loss: 1.6269
Epoch 4/10
2/2 [==============================] - 0s 2ms/step - loss: 1.6055
Epoch 5/10
2/2 [==============================] - 0s 2ms/step - loss: 1.5793
Epoch 6/10
2/2 [==============================] - 0s 2ms/step - loss: 1.5561
Epoch 7/10
2/2 [==============================] - 0s 2ms/step - loss: 1.5300
Epoch 8/10
2/2 [==============================] - 0s 997us/step - loss: 1.5064
Epoch 9/10
2/2 [==============================] - 0s 2ms/step - loss: 1.4795
Epoch 10/10
2/2 [==============================] - 0s 997us/step - loss: 1.4620
<keras.callbacks.History at 0x1f7eb24e520>
```

Epoch 위의 숫자 2/2 이 보여주듯이 전체 데이터를 2등분하여 학습합니다.

공식 ) 데이터의수(6)/배치사이즈(3) = 2

### 4. (추가) 배치 사이즈를 지정하지 않았을때

텐서플로우 기준 1/1 인 **풀배치** 로 학습된다.
